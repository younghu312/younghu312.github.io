<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Leveraging Multimodal AI for Traffic Light Detection — Yang Hu</title>
  <meta name="description" content="Benchmarking SEAL and GPT-4V (plus YOLO baselines) for distant traffic light color detection in real-world scenes.">
  <link rel="stylesheet" href="../assets/css/styles.css" />
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <a class="brand" href="/">Yang Hu</a>
      <nav class="nav" aria-label="Main">
        <a href="../index.html#research">← Back</a>
      </nav>
    </div>
  </header>

  <main class="container article">
    <h1>Leveraging Multimodal AI for Traffic Light Detection</h1>
    <div class="meta">Research · New York University</div>

    <div class="prose">
      <!-- Brief Intro -->
      <p>
        We benchmarked multimodal models for recognizing the color of far-distance traffic lights in complex street scenes,
        comparing SEAL’s LLM-guided visual search (V*) with GPT-4V and YOLO baselines on curated datasets (S2TLD and Google Street View).
        The study probes small-object recognition, multi-light scenes, and “no-light” negatives, and explores hybrid pipelines
        that pair precise visual grounding with stronger VQA inference.
      </p>

      <h3 class="text-lg font-semibold"><a class="button" href="https://github.com/younghu312/vstar" target="_blank" rel="noopener">Code</a></h3> 
     
      <!-- Highlights -->
      <h3 class="text-lg font-semibold">Highlights</h3>
      <ul>
        <li><strong>SEAL advantage on small/far lights:</strong> Best on <em>Small Lights</em> and high-res street-view cases via guided search.</li>
        <li><strong>GPT-4V excels on easy cases:</strong> 100% on <em>Big Lights</em> where the signal is large and clear.</li>
        <li><strong>Hybrid idea:</strong> Using SEAL for localization plus a stronger VQA (e.g., GPT-4o-mini) improves “No Light” accuracy to 100% while keeping competitive results elsewhere.</li>
      </ul>

      <!-- Methods -->
      <h3 class="text-lg font-semibold">Methods</h3>
      <ul>
        <li><strong>SEAL (V*):</strong> LLM-guided visual search to crop candidate regions → VQA for color choice.</li>
        <li><strong>SEAL + GPT-4o-mini:</strong> stronger VQA for option selection over SEAL crops.</li>
        <li><strong>YOLO + GPT-4o-mini:</strong> object detector for crops + VQA on crops & scene.</li>
        <li><strong>GPT-4V direct:</strong> single-pass VQA on the whole image.</li>
      </ul>

      <!-- Results (concise) -->
      <h3 class="text-lg font-semibold">Results (Snapshot)</h3>
      <ul>
        <li><strong>Big Lights:</strong> GPT-4V = 100% accuracy; SEAL ≈ 93% (misses tied to VQA confidence).</li>
        <li><strong>Small Lights:</strong> SEAL = 100% accuracy; GPT-4V ≈ 87%.</li>
        <li><strong>Google Street View HD:</strong> SEAL ≈ 45% vs GPT-4o-mini ≈ 27%.</li>
        <li><strong>No Light:</strong> SEAL + GPT-4o-mini = 100% (vs 0% with SEAL’s original VQA).</li>
      </ul>

      <!-- Your Role & Contributions -->
      <h3 class="text-lg font-semibold">Role & Contributions</h3>
      <ul>
        <li>Co-designed the study and prepared portions of the benchmark datasets (S2TLD splits & Google Street View subsets); labeled and verified ground-truth options.</li>
        <li>Implemented Python tooling to run and log VQA trials; evaluated GPT-4V across all datasets and documented outcomes.</li>
        <li>Built data and experiment scaffolding for hybrid pipelines (SEAL crops → GPT-4o-mini option choosing) and compared against YOLO + GPT baselines.</li>
        <li>Attempted LLaVA fine-tuning for SEAL’s VQA head; documented GPU/checkpoint constraints and laid out next-step training plans.</li>
        <li>Co-authored proposal and final report; summarized findings and future work on guided search + stronger VQA integration.</li>
      </ul>

      <!-- Tech Stack -->
      <h3 class="text-lg font-semibold">Tech Stack</h3>
      <ul class="flex flex-wrap gap-2 text-sm">
        <li class="px-2 py-1 bg-gray-200 rounded">Python</li>
        <li class="px-2 py-1 bg-gray-200 rounded">PyTorch</li>
        <li class="px-2 py-1 bg-gray-200 rounded">OpenAI GPT-4V / GPT-4o-mini</li>
        <li class="px-2 py-1 bg-gray-200 rounded">YOLO</li>
        <li class="px-2 py-1 bg-gray-200 rounded">pandas</li>
        <li class="px-2 py-1 bg-gray-200 rounded">Matplotlib</li>
      </ul>

    </div>
  </main>

  <footer class="site-footer">
    <div class="container">© <span id="year"></span> Yang Hu</div>
  </footer>
  <script src="../assets/js/main.js"></script>
</body>
</html>
